{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight dtypes: {dtype('float32')}\n",
      "I/O dtypes: {dtype('float32')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lg/ltf4rt5x5_7b6r3d1ty50m3h0000gq/T/ipykernel_33686/491560778.py:9: DeprecationWarning: `mapping.TENSOR_TYPE_TO_NP_TYPE` is now deprecated and will be removed in a future release.To silence this warning, please use `helper.tensor_dtype_to_np_dtype` instead.\n",
      "  mapping.TENSOR_TYPE_TO_NP_TYPE[t.data_type]\n",
      "/var/folders/lg/ltf4rt5x5_7b6r3d1ty50m3h0000gq/T/ipykernel_33686/491560778.py:18: DeprecationWarning: `mapping.TENSOR_TYPE_TO_NP_TYPE` is now deprecated and will be removed in a future release.To silence this warning, please use `helper.tensor_dtype_to_np_dtype` instead.\n",
      "  io_types.add(mapping.TENSOR_TYPE_TO_NP_TYPE[t])\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import helper, mapping\n",
    "\n",
    "# load your model\n",
    "model = onnx.load(\"models/w600k_mbf.onnx\")\n",
    "\n",
    "# collect all initializer (weight) dtypes\n",
    "dtypes = {\n",
    "    mapping.TENSOR_TYPE_TO_NP_TYPE[t.data_type]\n",
    "    for t in model.graph.initializer\n",
    "}\n",
    "print(\"Weight dtypes:\", dtypes)\n",
    "\n",
    "# also check all graph inputs and outputs\n",
    "io_types = set()\n",
    "for io in list(model.graph.input) + list(model.graph.output):\n",
    "    t = io.type.tensor_type.elem_type\n",
    "    io_types.add(mapping.TENSOR_TYPE_TO_NP_TYPE[t])\n",
    "print(\"I/O dtypes:\", io_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input `input.1`: tensor(float)\n",
      "Output `443`: tensor(float)\n",
      "Output `468`: tensor(float)\n",
      "Output `493`: tensor(float)\n",
      "Output `446`: tensor(float)\n",
      "Output `471`: tensor(float)\n",
      "Output `496`: tensor(float)\n",
      "Output `449`: tensor(float)\n",
      "Output `474`: tensor(float)\n",
      "Output `499`: tensor(float)\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "sess = ort.InferenceSession(\"models/det_500m.onnx\")\n",
    "for inp in sess.get_inputs():\n",
    "    print(f\"Input `{inp.name}`: {inp.type}\")   # e.g. \"tensor(float)\"\n",
    "for out in sess.get_outputs():\n",
    "    print(f\"Output `{out.name}`: {out.type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 400 images from\n",
      "  /Users/yousufsarfraz_1/Downloads/archive (4)/images/train\n",
      "to\n",
      "  /Users/yousufsarfraz_1/Documents/GitHub/mds-25/downsizing_experimental/calibration_images_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# 1. Update these paths to your source and target directories:\n",
    "src_dir = '/Users/yousufsarfraz_1/Downloads/archive (4)/images/train'\n",
    "dst_dir = '/Users/yousufsarfraz_1/Documents/GitHub/mds-25/downsizing_experimental/calibration_images_v2'\n",
    "\n",
    "# Create target directory if it doesn't exist\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "# 2. Collect all .png files from the source directory\n",
    "png_files = [f for f in os.listdir(src_dir)]\n",
    "\n",
    "# 3. Randomly sample up to 400 files\n",
    "sample_count = min(400, len(png_files))\n",
    "sampled_files = random.sample(png_files, sample_count)\n",
    "\n",
    "# 4. Copy each sampled file to the target directory\n",
    "for filename in sampled_files:\n",
    "    src_path = os.path.join(src_dir, filename)\n",
    "    dst_path = os.path.join(dst_dir, filename)\n",
    "    shutil.copy(src_path, dst_path)\n",
    "\n",
    "print(f\"Copied {sample_count} images from\\n  {src_dir}\\nto\\n  {dst_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static INT8 quantized model saved to: /Users/yousufsarfraz_1/Documents/GitHub/mds-25/downsizing_experimental/det_500m.static_int8_v2.onnx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType\n",
    "\n",
    "# Paths\n",
    "model_input = \"/Users/yousufsarfraz_1/.insightface/models/buffalo_sc/det_500m.onnx\"\n",
    "model_output = \"/Users/yousufsarfraz_1/Documents/GitHub/mds-25/downsizing_experimental/det_500m.static_int8_v2.onnx\"\n",
    "calib_folder = \"/Users/yousufsarfraz_1/Documents/GitHub/mds-25/downsizing_experimental/calibration_images_v2\"\n",
    "\n",
    "# 1. Create an ONNX Runtime session to fetch the input name\n",
    "session = onnxruntime.InferenceSession(model_input, providers=[\"CPUExecutionProvider\"])\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# 2. Calibration data reader\n",
    "class FaceCalibReader(CalibrationDataReader):\n",
    "    def __init__(self, image_folder, input_name):\n",
    "        self.image_paths = [\n",
    "            os.path.join(image_folder, f)\n",
    "            for f in os.listdir(image_folder)\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "        ]\n",
    "        self.input_name = input_name\n",
    "        self.iter = iter(self.image_paths)\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            img_path = next(self.iter)\n",
    "            # Load image (BGR), resize, convert to RGB\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (640, 640))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            # Normalize to [0,1], transpose to (C,H,W), add batch dim\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "            img = np.transpose(img, (2, 0, 1))[None, ...]\n",
    "            return {self.input_name: img}\n",
    "        except StopIteration:\n",
    "            return None\n",
    "\n",
    "calib_reader = FaceCalibReader(calib_folder, input_name)\n",
    "\n",
    "# 3. Run static quantization\n",
    "quantize_static(\n",
    "    model_input=model_input,\n",
    "    model_output=model_output,\n",
    "    calibration_data_reader=calib_reader,\n",
    "    activation_type=QuantType.QInt8,\n",
    "    weight_type=QuantType.QInt8\n",
    ")\n",
    "\n",
    "print(f\"Static INT8 quantized model saved to: {model_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dynamic-quantized model to /Users/yousufsarfraz_1/Documents/GitHub/mds-25/downsizing_experimental/det_500m.dynamic_int8_v1.onnx\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "input_model  = \"/Users/yousufsarfraz_1/.insightface/models/buffalo_sc/det_500m.onnx\"\n",
    "output_model = \"/Users/yousufsarfraz_1/Documents/GitHub/mds-25/downsizing_experimental/det_500m.dynamic_int8_v1.onnx\"\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input       = input_model,\n",
    "    model_output      = output_model,\n",
    "    weight_type       = QuantType.QUInt8,    # QInt8 or QUInt8\n",
    ")\n",
    "print(\"Saved dynamic-quantized model to\", output_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensors:\n",
      "- Name: input.1, Shape: [1, 3, 'dynamic', 'dynamic']\n",
      "\n",
      "Output tensors:\n",
      "- Name: 443, Shape: [12800, 1]\n",
      "- Name: 468, Shape: [3200, 1]\n",
      "- Name: 493, Shape: [800, 1]\n",
      "- Name: 446, Shape: [12800, 4]\n",
      "- Name: 471, Shape: [3200, 4]\n",
      "- Name: 496, Shape: [800, 4]\n",
      "- Name: 449, Shape: [12800, 10]\n",
      "- Name: 474, Shape: [3200, 10]\n",
      "- Name: 499, Shape: [800, 10]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load your ONNX model\n",
    "model = onnx.load(\"det_500m.dynamic_int8_v1.onnx\")\n",
    "\n",
    "# Inspect input tensors\n",
    "print(\"Input tensors:\")\n",
    "for input_tensor in model.graph.input:\n",
    "    tensor_shape = [dim.dim_value if dim.dim_value else 'dynamic' \n",
    "                    for dim in input_tensor.type.tensor_type.shape.dim]\n",
    "    print(f\"- Name: {input_tensor.name}, Shape: {tensor_shape}\")\n",
    "\n",
    "# Inspect output tensors\n",
    "print(\"\\nOutput tensors:\")\n",
    "for output_tensor in model.graph.output:\n",
    "    tensor_shape = [dim.dim_value if dim.dim_value else 'dynamic' \n",
    "                    for dim in output_tensor.type.tensor_type.shape.dim]\n",
    "    print(f\"- Name: {output_tensor.name}, Shape: {tensor_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yolov11.onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov11.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the ONNX model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Print Input Tensors\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYOLOv11 Input Tensors:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/mds-25/.venv/lib/python3.10/site-packages/onnx/__init__.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(f, format, load_external_data)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model\u001b[39m(\n\u001b[1;32m    192\u001b[0m     f: IO[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike,\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mformat\u001b[39m: _SupportedFormat \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# noqa: A002\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     load_external_data: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    195\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelProto:\n\u001b[1;32m    196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a serialized ModelProto into memory.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        Loaded in-memory ModelProto.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     model \u001b[38;5;241m=\u001b[39m _get_serializer(\u001b[38;5;28mformat\u001b[39m, f)\u001b[38;5;241m.\u001b[39mdeserialize_proto(\u001b[43m_load_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m, ModelProto())\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m load_external_data:\n\u001b[1;32m    215\u001b[0m         model_filepath \u001b[38;5;241m=\u001b[39m _get_file_path(f)\n",
      "File \u001b[0;32m~/Documents/GitHub/mds-25/.venv/lib/python3.10/site-packages/onnx/__init__.py:149\u001b[0m, in \u001b[0;36m_load_bytes\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     f \u001b[38;5;241m=\u001b[39m typing\u001b[38;5;241m.\u001b[39mcast(Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike], f)\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m readable:\n\u001b[1;32m    150\u001b[0m         content \u001b[38;5;241m=\u001b[39m readable\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yolov11.onnx'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Load a COCO-pretrained YOLO11n model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "\n",
    "# Create a dummy input tensor with the appropriate shape\n",
    "dummy_input = torch.randn(1, 3, 640, 640)\n",
    "\n",
    "# Trace the model with the dummy input\n",
    "traced_model = torch.jit.trace(model.model, dummy_input)\n",
    "\n",
    "# Print the traced model's graph to inspect input and output tensors\n",
    "print(traced_model.graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
